{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T04:28:38.584175Z",
     "iopub.status.busy": "2025-02-25T04:28:38.583220Z",
     "iopub.status.idle": "2025-02-25T04:28:39.654347Z",
     "shell.execute_reply": "2025-02-25T04:28:39.653526Z",
     "shell.execute_reply.started": "2025-02-25T04:28:38.584118Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\n",
      "/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv\n",
      "/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\n",
      "/kaggle/input/bnb-to-load-transformers-models/peft-0.11.1-py3-none-any.whl\n",
      "/kaggle/input/bnb-to-load-transformers-models/transformers-4.41.2-py3-none-any.whl\n",
      "/kaggle/input/bnb-to-load-transformers-models/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl\n",
      "/kaggle/input/bnb-to-load-transformers-models/accelerate-0.30.1-py3-none-any.whl\n",
      "/kaggle/input/huggingfacedebertav3variants/khalidalt-DeBERTa-v3-large/spm.model\n",
      "/kaggle/input/huggingfacedebertav3variants/khalidalt-DeBERTa-v3-large/config.json\n",
      "/kaggle/input/huggingfacedebertav3variants/khalidalt-DeBERTa-v3-large/README (1).md\n",
      "/kaggle/input/huggingfacedebertav3variants/khalidalt-DeBERTa-v3-large/README.md\n",
      "/kaggle/input/huggingfacedebertav3variants/khalidalt-DeBERTa-v3-large/tokenizer_config.json\n",
      "/kaggle/input/huggingfacedebertav3variants/khalidalt-DeBERTa-v3-large/tokenizer_config (1).json\n",
      "/kaggle/input/huggingfacedebertav3variants/khalidalt-DeBERTa-v3-large/pytorch_model.bin\n",
      "/kaggle/input/huggingfacedebertav3variants/khalidalt-DeBERTa-v3-large/special_tokens_map.json\n",
      "/kaggle/input/huggingfacedebertav3variants/khalidalt-DeBERTa-v3-large/gitattributes.txt\n",
      "/kaggle/input/huggingfacedebertav3variants/khalidalt-DeBERTa-v3-large/added_tokens.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-base-squad2/spm.model\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-base-squad2/config.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-base-squad2/README.md\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-base-squad2/tokenizer.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-base-squad2/tokenizer_config.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-base-squad2/pytorch_model.bin\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-base-squad2/special_tokens_map.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-base-squad2/gitattributes.txt\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-base-squad2/added_tokens.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-ontonotes5/spm.model\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-ontonotes5/config.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-ontonotes5/trainer_config.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-ontonotes5/README.md\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-ontonotes5/tokenizer.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-ontonotes5/tokenizer_config.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-ontonotes5/pytorch_model.bin\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-ontonotes5/special_tokens_map.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-ontonotes5/gitattributes.txt\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-ontonotes5/added_tokens.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-offensive/spm.model\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-offensive/run_test.sh\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-offensive/config.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-offensive/run_train.sh\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-offensive/trainer_state.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-offensive/eval_results.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-offensive/training_args.bin\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-offensive/README.md\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-offensive/tokenizer.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-offensive/all_results.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-offensive/tokenizer_config.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-offensive/pytorch_model.bin\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-offensive/special_tokens_map.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-offensive/gitattributes.txt\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-offensive/added_tokens.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large-offensive/test_results.json\n",
      "/kaggle/input/huggingfacedebertav3variants/yevheniimaslov-deberta-v3-base-cola/spm.model\n",
      "/kaggle/input/huggingfacedebertav3variants/yevheniimaslov-deberta-v3-base-cola/config.json\n",
      "/kaggle/input/huggingfacedebertav3variants/yevheniimaslov-deberta-v3-base-cola/trainer_state.json\n",
      "/kaggle/input/huggingfacedebertav3variants/yevheniimaslov-deberta-v3-base-cola/training_args.bin\n",
      "/kaggle/input/huggingfacedebertav3variants/yevheniimaslov-deberta-v3-base-cola/tokenizer.json\n",
      "/kaggle/input/huggingfacedebertav3variants/yevheniimaslov-deberta-v3-base-cola/tokenizer_config.json\n",
      "/kaggle/input/huggingfacedebertav3variants/yevheniimaslov-deberta-v3-base-cola/pytorch_model.bin\n",
      "/kaggle/input/huggingfacedebertav3variants/yevheniimaslov-deberta-v3-base-cola/scaler.pt\n",
      "/kaggle/input/huggingfacedebertav3variants/yevheniimaslov-deberta-v3-base-cola/scheduler.pt\n",
      "/kaggle/input/huggingfacedebertav3variants/yevheniimaslov-deberta-v3-base-cola/special_tokens_map.json\n",
      "/kaggle/input/huggingfacedebertav3variants/yevheniimaslov-deberta-v3-base-cola/optimizer.pt\n",
      "/kaggle/input/huggingfacedebertav3variants/yevheniimaslov-deberta-v3-base-cola/gitattributes.txt\n",
      "/kaggle/input/huggingfacedebertav3variants/yevheniimaslov-deberta-v3-base-cola/rng_state.pth\n",
      "/kaggle/input/huggingfacedebertav3variants/yevheniimaslov-deberta-v3-base-cola/added_tokens.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-base/rust_model.ot\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-base/spm.model\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-base/config.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-base/README.md\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-base/tf_model.h5\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-base/tokenizer_config.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-base/pytorch_model.bin\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-base/gitattributes.txt\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-xsmall/spm.model\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-xsmall/config.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-xsmall/pytorch_model.generator.bin\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-xsmall/README.md\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-xsmall/tf_model.h5\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-xsmall/tokenizer_config.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-xsmall/pytorch_model.bin\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-xsmall/generator_config.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-xsmall/gitattributes.txt\n",
      "/kaggle/input/huggingfacedebertav3variants/mdeberta-v3-base/spm.model\n",
      "/kaggle/input/huggingfacedebertav3variants/mdeberta-v3-base/config.json\n",
      "/kaggle/input/huggingfacedebertav3variants/mdeberta-v3-base/README.md\n",
      "/kaggle/input/huggingfacedebertav3variants/mdeberta-v3-base/tf_model.h5\n",
      "/kaggle/input/huggingfacedebertav3variants/mdeberta-v3-base/tokenizer_config.json\n",
      "/kaggle/input/huggingfacedebertav3variants/mdeberta-v3-base/pytorch_model.bin\n",
      "/kaggle/input/huggingfacedebertav3variants/mdeberta-v3-base/gitattributes.txt\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-small/spm.model\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-small/config.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-small/README.md\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-small/tf_model.h5\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-small/tokenizer_config.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-small/pytorch_model.bin\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-small/gitattributes.txt\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large/spm.model\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large/config.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large/README.md\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large/tf_model.h5\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large/tokenizer_config.json\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large/pytorch_model.bin\n",
      "/kaggle/input/huggingfacedebertav3variants/deberta-v3-large/gitattributes.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Install the necessary libraries for Parameter Efficient Fine-Tuning (PEFT) and model loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T06:06:36.629681Z",
     "iopub.status.busy": "2025-02-25T06:06:36.629037Z",
     "iopub.status.idle": "2025-02-25T06:08:38.672707Z",
     "shell.execute_reply": "2025-02-25T06:08:38.671644Z",
     "shell.execute_reply.started": "2025-02-25T06:06:36.629656Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/bnb-to-load-transformers-models/peft-0.11.1-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (5.9.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (2.1.2)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (4.39.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (4.66.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (0.30.1)\n",
      "Requirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.11.1) (0.22.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (2024.2.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft==0.11.1) (4.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft==0.11.1) (3.1.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.1) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.1) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.11.1) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.11.1) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft==0.11.1) (0.15.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.11.1) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.1) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.11.1) (1.3.0)\n",
      "peft is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install -q /kaggle/input/bnb-to-load-transformers-models/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl\n",
    "!pip install -q /kaggle/input/bnb-to-load-transformers-models/accelerate-0.30.1-py3-none-any.whl\n",
    "!pip install /kaggle/input/bnb-to-load-transformers-models/peft-0.11.1-py3-none-any.whl\n",
    "#!pip install -q /kaggle/input/bnb-to-load-transformers-models/transformers-4.41.2-py3-none-any.whl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T04:52:46.726372Z",
     "iopub.status.busy": "2025-02-25T04:52:46.726039Z",
     "iopub.status.idle": "2025-02-25T04:52:46.731170Z",
     "shell.execute_reply": "2025-02-25T04:52:46.730227Z",
     "shell.execute_reply.started": "2025-02-25T04:52:46.726346Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')  # Ignore warnings to keep the output clean\n",
    "\n",
    "# Importing essential libraries for computation and machine learning\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "from sklearn.metrics import mean_absolute_percentage_error  # Metric for regression\n",
    "from sklearn.model_selection import StratifiedKFold  # For performing cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and Configuration for Model Training and Fine-Tuning (PEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T04:52:50.228299Z",
     "iopub.status.busy": "2025-02-25T04:52:50.227976Z",
     "iopub.status.idle": "2025-02-25T04:52:50.232560Z",
     "shell.execute_reply": "2025-02-25T04:52:50.231670Z",
     "shell.execute_reply.started": "2025-02-25T04:52:50.228276Z"
    }
   },
   "outputs": [],
   "source": [
    "# HuggingFace libraries for model loading, training, and tokenization\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T04:52:51.684830Z",
     "iopub.status.busy": "2025-02-25T04:52:51.684261Z",
     "iopub.status.idle": "2025-02-25T04:52:51.690634Z",
     "shell.execute_reply": "2025-02-25T04:52:51.689777Z",
     "shell.execute_reply.started": "2025-02-25T04:52:51.684790Z"
    }
   },
   "outputs": [],
   "source": [
    "# Libraries for Parameter-Efficient Fine-Tuning (PEFT)\n",
    "from peft import (\n",
    "    LoftQConfig,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    "    PeftModel,\n",
    "    PeftConfig\n",
    ")\n",
    "\n",
    "# Configuration class containing all hyperparameters and setup details\n",
    "class CFG:\n",
    "    # Number of labels to predict. For regression, we have only one output value (the score).\n",
    "    n_labels = 1  \n",
    "    \n",
    "    # Set device to GPU if available, otherwise use CPU.\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
    "    \n",
    "    # Random seed for reproducibility of results.\n",
    "    seed = 1  \n",
    "    \n",
    "    # Path to the pre-trained model checkpoint (DeBERTa in this case).\n",
    "    model_ckpt = '/kaggle/input/huggingfacedebertav3variants/deberta-v3-base'\n",
    "    \n",
    "    # Hyperparameters for training:\n",
    "    max_input_length = 2000  # Maximum number of tokens the model can handle for each input\n",
    "    use_peft = False  # Flag to decide whether to use Parameter Efficient Fine-Tuning (PEFT)\n",
    "    n_freeze = None  # Number of transformer layers to freeze. Set to None to not freeze any layers\n",
    "    n_folds = 2  # Number of folds for Stratified KFold Cross-Validation\n",
    "    learning_rate = 5.0e-5  # Learning rate for model optimization\n",
    "    warmup_ratio = 0.1  # Warm-up ratio for learning rate scheduler\n",
    "    n_epochs = 1  # Number of epochs for training\n",
    "    train_batch_size = 4  # Batch size for training\n",
    "    eval_batch_size = 1  # Batch size for evaluation\n",
    "    grad_accum_steps = 4  # Number of steps to accumulate gradients before performing a backward pass\n",
    "    steps = 200  # Number of steps between logging or saving the model\n",
    "    fp16 = True  # Use 16-bit precision to reduce memory usage and speed up training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T04:52:56.344430Z",
     "iopub.status.busy": "2025-02-25T04:52:56.343795Z",
     "iopub.status.idle": "2025-02-25T04:52:57.561170Z",
     "shell.execute_reply": "2025-02-25T04:52:57.560422Z",
     "shell.execute_reply.started": "2025-02-25T04:52:56.344404Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "DATA_DIR = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2/'  # Dataset location\n",
    "df = pd.read_csv(DATA_DIR + 'train.csv')  # Read the training data from a CSV file\n",
    "\n",
    "# Convert the score to a continuous variable for regression\n",
    "df['label'] = df['score'] * 10  # Scaling score to continuous range\n",
    "df['label'] = df['label'].astype('float32')  # Ensuring the label is float for regression task\n",
    "\n",
    "# Check the distribution of the labels\n",
    "df['label'].value_counts()\n",
    "\n",
    "\n",
    "# Initialize the tokenizer from the pre-trained model checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_ckpt)\n",
    "\n",
    "# Create a data collator for padding sequences to the same length during batching\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T04:53:02.346971Z",
     "iopub.status.busy": "2025-02-25T04:53:02.346372Z",
     "iopub.status.idle": "2025-02-25T04:53:02.351789Z",
     "shell.execute_reply": "2025-02-25T04:53:02.350859Z",
     "shell.execute_reply.started": "2025-02-25T04:53:02.346943Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to tokenize the input text\n",
    "def tokenize(batch):\n",
    "    \"\"\"\n",
    "    This function takes a batch of text data and tokenizes it using the tokenizer \n",
    "    from the pre-trained DeBERTa model.\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        batch['full_text'],  # The input text column in the DataFrame\n",
    "        padding=False,  # Padding is handled by the data collator during training\n",
    "        truncation=True,  # Ensure that the text doesn't exceed the max input length\n",
    "        max_length=CFG.max_input_length,  # Truncate input to the maximum length\n",
    "    )\n",
    "    return tokenized_inputs  # Return the tokenized inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Initialization with Optional Fine-Tuning (PEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T04:53:03.039081Z",
     "iopub.status.busy": "2025-02-25T04:53:03.038720Z",
     "iopub.status.idle": "2025-02-25T04:53:03.045620Z",
     "shell.execute_reply": "2025-02-25T04:53:03.044870Z",
     "shell.execute_reply.started": "2025-02-25T04:53:03.039057Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to initialize the model with optional fine-tuning\n",
    "def model_init():\n",
    "    \"\"\"\n",
    "    This function initializes the DeBERTa model and configures it for the regression task.\n",
    "    It also handles freezing layers if specified and applies PEFT (if enabled).\n",
    "    \"\"\"\n",
    "    # Load the model from the pre-trained checkpoint with the specified number of output labels\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        CFG.model_ckpt, \n",
    "        num_labels=CFG.n_labels,\n",
    "    ).to(CFG.device)  # Move the model to the specified device (GPU or CPU)\n",
    "    \n",
    "    # Freeze the embedding and transformer layers if specified (for transfer learning)\n",
    "    if CFG.n_freeze is not None:\n",
    "        for param in model.base_model.embeddings.parameters():\n",
    "            param.requires_grad = False  # Freeze embedding layer\n",
    "        for i in range(CFG.n_freeze):  # Freeze the specified number of transformer layers\n",
    "            for param in model.base_model.encoder.layer[i].parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    # Apply Parameter Efficient Fine-Tuning (PEFT) if enabled\n",
    "    if CFG.use_peft:\n",
    "        loftq_config = LoftQConfig(loftq_bits=4)  # PEFT configuration using LoftQ quantization\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,  # Sequence classification task (used for regression)\n",
    "            inference_mode=False,  # Training mode (inference would be true during actual inference)\n",
    "            init_lora_weights='loftq',  # Use LoftQ initialization for PEFT\n",
    "            loftq_config=loftq_config,  # LoftQ configuration\n",
    "            use_rslora=True,  # Use RSLora for low-rank adaptation\n",
    "            r=16,  # Rank for low-rank adaptation\n",
    "            lora_alpha=8,  # Alpha scaling factor for LoRA layers\n",
    "            lora_dropout=0,  # No dropout in LoRA layers\n",
    "        )\n",
    "        model = get_peft_model(model, peft_config)  # Apply PEFT to the model\n",
    "    \n",
    "    return model  # Return the initialized model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metric, Seed Setting, and Trainable Parameter Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T04:53:06.611227Z",
     "iopub.status.busy": "2025-02-25T04:53:06.610650Z",
     "iopub.status.idle": "2025-02-25T04:53:06.617943Z",
     "shell.execute_reply": "2025-02-25T04:53:06.617035Z",
     "shell.execute_reply.started": "2025-02-25T04:53:06.611199Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the Mean Absolute Percentage Error (MAPE) metric for evaluation\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    This function calculates the Mean Absolute Percentage Error (MAPE) \n",
    "    between the true labels and the model predictions.\n",
    "    \"\"\"\n",
    "    labels = pred.label_ids  # True labels from the evaluation set\n",
    "    predictions = pred.predictions  # Predictions from the model\n",
    "    mape = mean_absolute_percentage_error(labels, predictions)  # Calculate MAPE\n",
    "    return {'mape': mape}  # Return the MAPE value\n",
    "\n",
    "\n",
    "# Function to set the random seed for reproducibility\n",
    "def seed_everything(seed: int):\n",
    "    \"\"\"\n",
    "    This function sets the seed for all random number generators (Python, NumPy, PyTorch, CUDA) \n",
    "    to ensure reproducibility of results.\n",
    "    \"\"\"\n",
    "    random.seed(seed)  # Set seed for Python's random module\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)  # Set seed for Python's hash function\n",
    "    np.random.seed(seed)  # Set seed for NumPy\n",
    "    torch.manual_seed(seed)  # Set seed for PyTorch\n",
    "    torch.cuda.manual_seed(seed)  # Set seed for CUDA (if using GPU)\n",
    "    torch.backends.cudnn.deterministic = True  # Ensure deterministic results\n",
    "    torch.backends.cudnn.benchmark = False  # Disable non-deterministic algorithms\n",
    "\n",
    "\n",
    "# Function to print the number of trainable parameters in the model\n",
    "def print_trainable_params(model):\n",
    "    \"\"\"\n",
    "    This function prints the number of trainable parameters and the total number of parameters \n",
    "    in the model, as well as the ratio between them.\n",
    "    \"\"\"\n",
    "    trainable_params = 0  # Initialize count for trainable parameters\n",
    "    all_params = 0  # Initialize count for all parameters in the model\n",
    "    \n",
    "    # Iterate over the model parameters and count the trainable ones\n",
    "    for _, param in model.named_parameters():\n",
    "        all_params += param.numel()  # Add total number of parameters\n",
    "        if param.requires_grad == True:  # Check if the parameter is trainable\n",
    "            trainable_params += param.numel()  # Add to the trainable parameter count\n",
    "    \n",
    "    # Print the counts and the ratio\n",
    "    print(f\"trainable parameters: {trainable_params}, all parameters: {all_params}, ratio: {100 * trainable_params / all_params}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"model-initialization-with-optional-fine-tuning-peft\"></a>\n",
    "## Model Initialization, Cross-Validation, and Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T04:53:11.536708Z",
     "iopub.status.busy": "2025-02-25T04:53:11.536100Z",
     "iopub.status.idle": "2025-02-25T06:01:12.646096Z",
     "shell.execute_reply": "2025-02-25T06:01:12.645301Z",
     "shell.execute_reply.started": "2025-02-25T04:53:11.536681Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/huggingfacedebertav3variants/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable parameters: 184422913, all parameters: 184422913, ratio: 100.0%\n",
      "######################### Fold 0 #########################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f752af919f8442b8eff4885943fa05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8653 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be5e2fe9a704a0e82576054bf777a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='541' max='541' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [541/541 33:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>386.526100</td>\n",
       "      <td>105.185226</td>\n",
       "      <td>0.416124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>56.921000</td>\n",
       "      <td>68.126259</td>\n",
       "      <td>0.310461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################### Fold 1 #########################\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7b289cd7bf4a9f8677161f04e605ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8654 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f97e288b1e44101bd70c1b26ac073a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8653 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='541' max='541' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [541/541 33:43, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>47.483100</td>\n",
       "      <td>40.639767</td>\n",
       "      <td>0.221849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>41.886400</td>\n",
       "      <td>43.680431</td>\n",
       "      <td>0.218837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = model_init()\n",
    "\n",
    "# Print the number of trainable parameters\n",
    "print_trainable_params(model)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed_everything(CFG.seed)\n",
    "\n",
    "\n",
    "# Cross-validation using Stratified KFold\n",
    "skf = StratifiedKFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.seed)  # Stratified KFold setup for splitting data\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(df, df['label'])):\n",
    "    # Split the dataset into training and validation sets\n",
    "    df_train = df.loc[tr_idx, ['full_text', 'label']].copy()  # Training data\n",
    "    df_valid = df.loc[va_idx, ['full_text', 'label']].copy()  # Validation data\n",
    "    print('#'*25, f\"Fold {fold}\", '#'*25)  # Print fold information\n",
    "    \n",
    "    # Prepare datasets using HuggingFace Datasets library\n",
    "    ds_train = datasets.Dataset.from_pandas(df_train)  # Convert to HuggingFace dataset format\n",
    "    ds_valid = datasets.Dataset.from_pandas(df_valid)  # Convert to HuggingFace dataset format\n",
    "    \n",
    "    # Tokenize the datasets\n",
    "    tokenized_ds_train = ds_train.map(tokenize, batched=True, batch_size=None)\n",
    "    tokenized_ds_valid = ds_valid.map(tokenize, batched=True, batch_size=None)\n",
    "    \n",
    "    # Convert datasets to PyTorch format\n",
    "    tokenized_ds_train.set_format('torch')\n",
    "    tokenized_ds_valid.set_format('torch')\n",
    "    \n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='/kaggle/temp/',  # Directory to save model checkpoints\n",
    "        overwrite_output_dir=True,  # Allow overwriting the output directory\n",
    "        learning_rate=CFG.learning_rate,  # Learning rate\n",
    "        warmup_ratio=CFG.warmup_ratio,  # Warm-up ratio for learning rate\n",
    "        num_train_epochs=CFG.n_epochs,  # Number of epochs to train the model\n",
    "        per_device_train_batch_size=CFG.train_batch_size,  # Batch size for training\n",
    "        per_device_eval_batch_size=CFG.eval_batch_size,  # Batch size for evaluation\n",
    "        gradient_accumulation_steps=CFG.grad_accum_steps,  # Gradient accumulation steps\n",
    "        gradient_checkpointing=True,  # Use gradient checkpointing for memory efficiency\n",
    "        fp16=CFG.fp16,  # Enable FP16 precision for faster computation\n",
    "        logging_strategy='steps',  # Log every few steps\n",
    "        logging_steps=CFG.steps,  # Number of steps between logging\n",
    "        evaluation_strategy='steps',  # Evaluate model every few steps\n",
    "        eval_steps=CFG.steps,  # Number of steps between evaluations\n",
    "        save_strategy='steps',  # Save model every few steps\n",
    "        save_steps=CFG.steps,  # Number of steps between saving\n",
    "        save_total_limit=1,  # Keep only the most recent checkpoint\n",
    "        load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "        report_to='none',  # Disable reporting (use 'tensorboard' for logging)\n",
    "        seed=CFG.seed,  # Set the random seed for training\n",
    "    )\n",
    "    \n",
    "    # Initialize the trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,  # Model to train\n",
    "        args=training_args,  # Training arguments\n",
    "        train_dataset=tokenized_ds_train,  # Training dataset\n",
    "        eval_dataset=tokenized_ds_valid,  # Validation dataset\n",
    "        tokenizer=tokenizer,  # Tokenizer for text preprocessing\n",
    "        data_collator=data_collator,  # Data collator for batching\n",
    "        compute_metrics=compute_metrics,  # Metrics for evaluation\n",
    "    )\n",
    "    \n",
    "    # Start the training process\n",
    "    trainer.train()\n",
    "    \n",
    "    # Only perform one fold to save time. Uncomment 'break' to perform cross-validation with multiple folds.\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8059942,
     "sourceId": 71485,
     "sourceType": "competition"
    },
    {
     "datasetId": 2663421,
     "sourceId": 4620664,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5127900,
     "sourceId": 8575516,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
