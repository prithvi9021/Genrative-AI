{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-25T06:42:32.169877Z",
     "iopub.status.busy": "2025-02-25T06:42:32.169430Z",
     "iopub.status.idle": "2025-02-25T06:42:32.190271Z",
     "shell.execute_reply": "2025-02-25T06:42:32.189271Z",
     "shell.execute_reply.started": "2025-02-25T06:42:32.169848Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/companypolicies/company_policy.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Required Libraries for Model and Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T06:42:32.191565Z",
     "iopub.status.busy": "2025-02-25T06:42:32.191286Z",
     "iopub.status.idle": "2025-02-25T06:42:38.717868Z",
     "shell.execute_reply": "2025-02-25T06:42:38.716948Z",
     "shell.execute_reply.started": "2025-02-25T06:42:32.191542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: faiss-cpu in /root/.local/lib/python3.10/site-packages (1.10.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy<2,>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib) (2.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.20->matplotlib) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.20->matplotlib) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1.20->matplotlib) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2,>=1.20->matplotlib) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2,>=1.20->matplotlib) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries for transformer-based models, datasets, and PyTorch (for deep learning)\n",
    "!pip install --user transformers datasets torch faiss-cpu \n",
    "\n",
    "# Install additional libraries for data visualization and machine learning\n",
    "!pip install --user matplotlib scikit-learn \n",
    "\n",
    "# Install FAISS with GPU support for efficient similarity search and clustering\n",
    "!pip install faiss-gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T06:42:38.719787Z",
     "iopub.status.busy": "2025-02-25T06:42:38.719552Z",
     "iopub.status.idle": "2025-02-25T06:42:38.725684Z",
     "shell.execute_reply": "2025-02-25T06:42:38.724713Z",
     "shell.execute_reply.started": "2025-02-25T06:42:38.719766Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "# 'wget' is commented out, but you can use it to download files from URLs if needed\n",
    "# import wget  \n",
    "\n",
    "# Importing transformers for working with the Dense Retriever (DPR) model\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer  # For encoding context using DPR\n",
    "import torch  # PyTorch for deep learning models\n",
    "\n",
    "# Importing additional libraries for randomization and number operations\n",
    "import numpy as np  # For numerical operations with arrays\n",
    "import random  # For generating random numbers\n",
    "\n",
    "# Importing question encoding modules from HuggingFace's transformers library\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer  # For encoding questions using DPR\n",
    "\n",
    "# Importing AutoTokenizer and AutoModel for pre-trained causal language model (e.g., GPT-2 or GPT-3)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM  # For text generation models (causal LM)\n",
    "\n",
    "# Importing libraries for 3D plotting and dimensionality reduction\n",
    "import matplotlib.pyplot as plt  # For plotting graphs (used for visualizations)\n",
    "from mpl_toolkits.mplot3d import Axes3D  # For 3D plotting support in matplotlib\n",
    "\n",
    "# Importing t-SNE for dimensionality reduction (commonly used for visualizing high-dimensional data)\n",
    "from sklearn.manifold import TSNE  # For dimensionality reduction to visualize high-dimensional data\n",
    "\n",
    "# Importing numpy again (redundant as it's already imported) â€“ this could be removed\n",
    "import numpy as np  # For numerical operations (again, already imported)\n",
    "\n",
    "# Function to suppress warnings during execution\n",
    "def warn(*args, **kwargs):\n",
    "    pass  # This function does nothing, effectively suppressing warnings\n",
    "\n",
    "# Reassigning 'warnings.warn' to suppress all warnings\n",
    "import warnings  # Import warnings module\n",
    "warnings.warn = warn  # Override the default 'warn' function to suppress warnings\n",
    "warnings.filterwarnings('ignore')  # Ignore all warnings in the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing, Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T06:42:38.727124Z",
     "iopub.status.busy": "2025-02-25T06:42:38.726866Z",
     "iopub.status.idle": "2025-02-25T06:42:38.749405Z",
     "shell.execute_reply": "2025-02-25T06:42:38.748533Z",
     "shell.execute_reply.started": "2025-02-25T06:42:38.727104Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.\\tCode of Conduct',\n",
       " 'Our Code of Conduct outlines the fundamental principles and ethical standards that guide every member of our organization. We are committed to maintaining a workplace that is built on integrity, respect, and accountability.',\n",
       " 'Integrity: We hold ourselves to the highest ethical standards. This means acting honestly and transparently in all our interactions, whether with colleagues, clients, or the broader community. We respect and protect sensitive information, and we avoid conflicts of interest.',\n",
       " \"Respect: We embrace diversity and value each individual's contributions. Discrimination, harassment, or any form of disrespectful behavior is unacceptable. We create an inclusive environment where differences are celebrated and everyone is treated with dignity and courtesy.\",\n",
       " 'Accountability: We take responsibility for our actions and decisions. We follow all relevant laws and regulations, and we strive to continuously improve our practices. We report any potential violations of this code and support the investigation of such matters.',\n",
       " 'Safety: We prioritize the safety of our employees, clients, and the communities we serve. We maintain a culture of safety, including reporting any unsafe conditions or practices.',\n",
       " 'Environmental Responsibility: We are committed to minimizing our environmental footprint and promoting sustainable practices.',\n",
       " \"Our Code of Conduct is not just a set of rules; it is the foundation of our organization's culture. We expect all employees to uphold these principles and serve as role models for others, ensuring we maintain our reputation for ethical conduct, integrity, and social responsibility.\",\n",
       " '2.\\tRecruitment Policy',\n",
       " 'Our Recruitment Policy reflects our commitment to attracting, selecting, and onboarding the most qualified and diverse candidates to join our organization. We believe that the success of our company relies on the talents, skills, and dedication of our employees.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_and_split_text(filename):\n",
    "    \"\"\"\n",
    "    Reads a text file and splits it into non-empty paragraphs.\n",
    "    \n",
    "    Args:\n",
    "    - filename (str): The path to the text file to be read.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of non-empty paragraphs from the file.\n",
    "    \"\"\"\n",
    "    # Open the file and read its content\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Split the text by newlines and filter out empty paragraphs\n",
    "    paragraphs = [para.strip() for para in text.split('\\n') if para.strip()]\n",
    "    \n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "# Define the file path\n",
    "file_path = '/kaggle/input/companypolicies/company_policy.txt'\n",
    "\n",
    "# Read the file and get the paragraphs\n",
    "paragraphs = read_and_split_text(file_path)\n",
    "\n",
    "# Display the first 10 paragraphs (for preview)\n",
    "paragraphs[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T06:45:25.904298Z",
     "iopub.status.busy": "2025-02-25T06:45:25.903975Z",
     "iopub.status.idle": "2025-02-25T06:45:25.909841Z",
     "shell.execute_reply": "2025-02-25T06:45:25.908932Z",
     "shell.execute_reply.started": "2025-02-25T06:45:25.904274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0: Treatment and Assistance: Employees with substance abuse issues are encouraged to seek help. The organization is committed to providing support, resources, and information to assist those seeking treatment.\n",
      "\n",
      "Sample 1: Cost Management: Keep personal phone usage separate from company accounts and reimburse the company for any personal charges on company-issued phones.\n",
      "\n",
      "Sample 2: Security: Safeguard your mobile device and access credentials. Exercise caution when downloading apps or clicking links from unfamiliar sources. Promptly report security concerns or suspicious activities related to your mobile device.\n",
      "\n",
      "Sample 3: Harassment: Harassment in any form, whether based on the aforementioned characteristics or any other protected status, is unacceptable. This encompasses unwelcome advances, offensive jokes, slurs, and other verbal or physical conduct that creates a hostile or intimidating work environment.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through the first 4 paragraphs and print them with an index\n",
    "for i in range(4):\n",
    "    # Print the index and corresponding paragraph with clear formatting\n",
    "    print(f\"Sample {i}: {paragraphs[i]}\\n\")  # Display paragraph with its index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T06:47:26.673743Z",
     "iopub.status.busy": "2025-02-25T06:47:26.673386Z",
     "iopub.status.idle": "2025-02-25T06:47:26.998026Z",
     "shell.execute_reply": "2025-02-25T06:47:26.997103Z",
     "shell.execute_reply.started": "2025-02-25T06:47:26.673712Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# Load the DPR (Dense Passage Retriever) context tokenizer for a pre-trained model\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "# Display the tokenizer object (this won't be shown due to %%capture)\n",
    "context_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T06:52:57.380755Z",
     "iopub.status.busy": "2025-02-25T06:52:57.380339Z",
     "iopub.status.idle": "2025-02-25T06:52:58.557347Z",
     "shell.execute_reply": "2025-02-25T06:52:58.556374Z",
     "shell.execute_reply.started": "2025-02-25T06:52:57.380719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('How are you?', 'I am fine.'), (\"What's up?\", 'Not much.')]\n",
      "['[CLS]', 'how', 'are', 'you', '?', '[SEP]', 'i', 'am', 'fine', '.', '[SEP]']\n",
      "['[CLS]', 'what', \"'\", 's', 'up', '?', '[SEP]', 'not', 'much', '.', '[SEP]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Define a list of tuples with question-answer pairs\n",
    "text = [(\"How are you?\", \"I am fine.\"), (\"What's up?\", \"Not much.\")]\n",
    "\n",
    "# Print the list of question-answer pairs\n",
    "print(text)\n",
    "\n",
    "# Tokenize the input text using the context tokenizer, \n",
    "# with padding, truncation, and a maximum length of 256 tokens.\n",
    "tokens_info = context_tokenizer(\n",
    "    text, \n",
    "    return_tensors='pt',  # Return PyTorch tensors for model input\n",
    "    padding=True,  # Pad sequences to the longest one in the batch\n",
    "    truncation=True,  # Truncate sequences longer than max_length\n",
    "    max_length=256  # Maximum sequence length of 256 tokens\n",
    ")\n",
    "\n",
    "# Display the tokenized information (input IDs and attention masks)\n",
    "tokens_info\n",
    "\n",
    "# Iterate over each tokenized input and convert its ID back to tokens\n",
    "for s in tokens_info['input_ids']:\n",
    "    # Convert token IDs to readable tokens and print them\n",
    "    print(context_tokenizer.convert_ids_to_tokens(s))\n",
    "\n",
    "# Load the pre-trained DPR (Dense Passage Retrieval) context encoder model\n",
    "context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T07:14:39.022659Z",
     "iopub.status.busy": "2025-02-25T07:14:39.022321Z",
     "iopub.status.idle": "2025-02-25T07:14:40.199777Z",
     "shell.execute_reply": "2025-02-25T07:14:40.198949Z",
     "shell.execute_reply.started": "2025-02-25T07:14:39.022632Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3869,  0.4322,  0.3379,  ..., -0.4071, -0.0797, -0.0184],\n",
       "        [ 0.3005,  0.5121, -0.1547,  ..., -0.7096, -0.9071,  0.2282],\n",
       "        [ 0.2214,  0.3090, -0.3535,  ..., -0.4585, -0.9006, -0.1875],\n",
       "        ...,\n",
       "        [ 0.1780,  0.5352,  0.5585,  ..., -0.6085, -0.8304, -0.1766],\n",
       "        [ 0.4561,  0.3122,  0.1356,  ..., -0.2938, -0.4350,  0.0328],\n",
       "        [ 0.6020,  0.3124,  0.0835,  ..., -0.5222, -0.2321,  0.2006]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle the samples randomly to ensure they are not ordered based on their category\n",
    "# This helps prevent any bias during processing that could arise from an ordered dataset\n",
    "random.shuffle(paragraphs)\n",
    "\n",
    "# Tokenize the first 20 shuffled paragraphs for input into the model\n",
    "# The tokenization process includes padding, truncating to a maximum length of 256 tokens\n",
    "tokens = context_tokenizer(paragraphs[:20], return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "\n",
    "# Display the tokenized output (in tensor format) for the first 20 paragraphs\n",
    "tokens\n",
    "\n",
    "# Pass the tokenized input through the context encoder to generate embeddings for the paragraphs\n",
    "# The encoder processes the tokenized input and returns the corresponding embeddings\n",
    "outputs = context_encoder(**tokens)\n",
    "\n",
    "# Extract the pooler_output (the representation of the entire text) from the encoder output\n",
    "# This output is typically used as the fixed-size representation of each paragraph\n",
    "outputs.pooler_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T06:42:43.198539Z",
     "iopub.status.busy": "2025-02-25T06:42:43.198219Z",
     "iopub.status.idle": "2025-02-25T06:42:43.202052Z",
     "shell.execute_reply": "2025-02-25T06:42:43.201113Z",
     "shell.execute_reply.started": "2025-02-25T06:42:43.198516Z"
    }
   },
   "outputs": [],
   "source": [
    "#tsne_plot(outputs.pooler_output.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Generation, and t-SNE Visualization of Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T07:03:19.354831Z",
     "iopub.status.busy": "2025-02-25T07:03:19.354398Z",
     "iopub.status.idle": "2025-02-25T07:03:19.788019Z",
     "shell.execute_reply": "2025-02-25T07:03:19.787193Z",
     "shell.execute_reply.started": "2025-02-25T07:03:19.354798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries for visualization and dimensionality reduction\n",
    "import matplotlib.pyplot as plt  # For plotting scatter plots\n",
    "from sklearn.manifold import TSNE  # For performing t-SNE (t-Distributed Stochastic Neighbor Embedding) for dimensionality reduction\n",
    "\n",
    "# Assuming `outputs.pooler_output` is the tensor containing the embeddings you want to visualize\n",
    "# Detach the tensor from the computation graph and convert it to a NumPy array for further processing\n",
    "embeddings = outputs.pooler_output.detach().numpy()\n",
    "\n",
    "# Perform t-SNE transformation to reduce dimensionality to 2D for visualization\n",
    "# n_components=2 specifies that we want to reduce the embeddings to 2D\n",
    "# perplexity is a hyperparameter that affects the balance between local and global aspects of the data\n",
    "tsne = TSNE(n_components=2, perplexity=5)\n",
    "reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "# Create a scatter plot of the 2D reduced embeddings\n",
    "plt.figure(figsize=(8, 6))  # Set the size of the plot\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1])  # Plot the 2D points\n",
    "plt.title(\"t-SNE Visualization\")  # Add a title to the plot\n",
    "plt.show()  # Display the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T06:42:43.607816Z",
     "iopub.status.busy": "2025-02-25T06:42:43.607606Z",
     "iopub.status.idle": "2025-02-25T06:42:44.349480Z",
     "shell.execute_reply": "2025-02-25T06:42:44.348584Z",
     "shell.execute_reply.started": "2025-02-25T06:42:43.607798Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples:\n",
      "1\n",
      " samples shape:\n",
      "torch.Size([1, 768])\n",
      "number of samples:\n",
      "2\n",
      " samples shape:\n",
      "torch.Size([1, 768])\n",
      "number of samples:\n",
      "3\n",
      " samples shape:\n",
      "torch.Size([1, 768])\n",
      "number of samples:\n",
      "4\n",
      " samples shape:\n",
      "torch.Size([1, 768])\n",
      "number of samples:\n",
      "5\n",
      " samples shape:\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the embeddings of the text samples\n",
    "embeddings = []\n",
    "\n",
    "# Iterate through the first 5 paragraphs to generate embeddings\n",
    "for text in paragraphs[0:5]:\n",
    "    # Tokenize each text using the context tokenizer\n",
    "    inputs = context_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "    \n",
    "    # Pass the tokenized inputs through the context encoder to generate the embedding\n",
    "    outputs = context_encoder(**inputs)\n",
    "    \n",
    "    # Append the pooler_output (embedding) to the embeddings list\n",
    "    embeddings.append(outputs.pooler_output)\n",
    "    \n",
    "    # Print the number of samples processed so far\n",
    "    print(\"Number of samples processed:\")\n",
    "    print(len(embeddings))  # Length of embeddings list represents the number of processed samples\n",
    "    \n",
    "    # Print the shape of the latest embedding (pooler_output)\n",
    "    print(\"Shape of the embedding for the current sample:\")\n",
    "    print(outputs.pooler_output.shape)  # Output shape of the current sample's embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T06:42:44.350765Z",
     "iopub.status.busy": "2025-02-25T06:42:44.350406Z",
     "iopub.status.idle": "2025-02-25T06:42:44.356190Z",
     "shell.execute_reply": "2025-02-25T06:42:44.355543Z",
     "shell.execute_reply.started": "2025-02-25T06:42:44.350730Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 768)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(embeddings).detach().numpy().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T07:58:07.260877Z",
     "iopub.status.busy": "2025-02-25T07:58:07.260481Z",
     "iopub.status.idle": "2025-02-25T07:58:07.265581Z",
     "shell.execute_reply": "2025-02-25T07:58:07.264370Z",
     "shell.execute_reply.started": "2025-02-25T07:58:07.260842Z"
    }
   },
   "source": [
    "#  FAISS Indexing, and Retrieval for Question-Context Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T07:12:02.708856Z",
     "iopub.status.busy": "2025-02-25T07:12:02.708549Z",
     "iopub.status.idle": "2025-02-25T07:12:08.619644Z",
     "shell.execute_reply": "2025-02-25T07:12:08.618927Z",
     "shell.execute_reply.started": "2025-02-25T07:12:02.708833Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_contexts(text_list):\n",
    "    \"\"\"\n",
    "    Encodes a list of text passages into embeddings using a pre-trained context encoder.\n",
    "\n",
    "    Args:\n",
    "    - text_list (list): A list of text passages (e.g., paragraphs) to be encoded.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The concatenated embeddings for all input texts.\n",
    "    \"\"\"\n",
    "    embeddings = []  # Initialize an empty list to store the embeddings\n",
    "    \n",
    "    # Loop through each text in the provided list\n",
    "    for text in text_list:\n",
    "        # Tokenize the text to prepare it for the context encoder\n",
    "        inputs = context_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "        \n",
    "        # Pass the tokenized inputs through the context encoder to get the embeddings\n",
    "        outputs = context_encoder(**inputs)\n",
    "        \n",
    "        # Extract the pooler output (final representation of the text) and append it to the embeddings list\n",
    "        embeddings.append(outputs.pooler_output)\n",
    "    \n",
    "    # Concatenate all embeddings into a single tensor, detach from computation graph, and convert to NumPy array\n",
    "    return torch.cat(embeddings).detach().numpy()\n",
    "\n",
    "\n",
    "# Encode the paragraphs into embeddings\n",
    "# The `paragraphs` list contains the text data you want to encode.\n",
    "context_embeddings = encode_contexts(paragraphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T06:42:58.894019Z",
     "iopub.status.busy": "2025-02-25T06:42:58.893786Z",
     "iopub.status.idle": "2025-02-25T06:42:58.899333Z",
     "shell.execute_reply": "2025-02-25T06:42:58.898401Z",
     "shell.execute_reply.started": "2025-02-25T06:42:58.893998Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the FAISS library, which is used for efficient similarity search\n",
    "import faiss\n",
    "\n",
    "# Set the dimension of the embeddings\n",
    "embedding_dim = 768  # This should match the dimension of your embeddings (e.g., for BERT-based embeddings)\n",
    "\n",
    "# Convert the list of context embeddings into a single NumPy array and ensure it's in float32 format\n",
    "context_embeddings_np = np.array(context_embeddings).astype('float32')\n",
    "\n",
    "# Create a FAISS index for performing fast similarity search using L2 (Euclidean) distance\n",
    "# IndexFlatL2 is a basic FAISS index that computes the L2 distance between vectors\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# Add the context embeddings to the FAISS index for future search operations\n",
    "index.add(context_embeddings_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T06:42:58.900645Z",
     "iopub.status.busy": "2025-02-25T06:42:58.900150Z",
     "iopub.status.idle": "2025-02-25T06:43:00.494438Z",
     "shell.execute_reply": "2025-02-25T06:43:00.493324Z",
     "shell.execute_reply.started": "2025-02-25T06:42:58.900612Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained DPR (Dense Passage Retrieval) question encoder and tokenizer\n",
    "# These are used for encoding questions and tokenizing them to prepare for retrieval tasks\n",
    "\n",
    "# Load the question encoder model from the HuggingFace library\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "\n",
    "# Load the tokenizer for the question encoder, which tokenizes the input question text\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T07:10:22.164239Z",
     "iopub.status.busy": "2025-02-25T07:10:22.163881Z",
     "iopub.status.idle": "2025-02-25T07:10:22.222572Z",
     "shell.execute_reply": "2025-02-25T07:10:22.221722Z",
     "shell.execute_reply.started": "2025-02-25T07:10:22.164206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances (D): [[72.76532 74.71617 84.38807 88.36439 90.28715]]\n",
      "Indices (I): [[74 38 12  7 24]]\n"
     ]
    }
   ],
   "source": [
    "# Define the example question for which we want to find relevant contexts\n",
    "question = 'Drug and Alcohol Policy'\n",
    "\n",
    "# Tokenize the question using the pre-defined tokenizer for the question\n",
    "question_inputs = question_tokenizer(question, return_tensors='pt')\n",
    "\n",
    "# Generate the embedding for the question using the question encoder\n",
    "# .pooler_output gives the pooled output representation of the question\n",
    "question_embedding = question_encoder(**question_inputs).pooler_output.detach().numpy()\n",
    "\n",
    "# Search the index for the top 5 most relevant contexts based on the question embedding\n",
    "D, I = index.search(question_embedding, k=5)  # D: distances, I: indices of the top 5 contexts\n",
    "\n",
    "# Print the distances and indices of the top 5 retrieved contexts\n",
    "print(\"Distances (D):\", D)\n",
    "print(\"Indices (I):\", I)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T06:43:00.572204Z",
     "iopub.status.busy": "2025-02-25T06:43:00.571893Z",
     "iopub.status.idle": "2025-02-25T06:43:00.579115Z",
     "shell.execute_reply": "2025-02-25T06:43:00.578327Z",
     "shell.execute_reply.started": "2025-02-25T06:43:00.572178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 relevant contexts:\n",
      "1: 6.\tDrug and Alcohol Policy\n",
      "distance 72.76531982421875\n",
      "\n",
      "2: Policy Objective: The Drug and Alcohol Policy is established to establish clear expectations and guidelines for the responsible use of drugs and alcohol within the organization. This policy aims to maintain a safe, healthy, and productive workplace.\n",
      "distance 74.71617126464844\n",
      "\n",
      "3: Testing and Searches: The organization reserves the right to conduct drug and alcohol testing as per applicable laws and regulations. Employees may be subject to testing in cases of reasonable suspicion, post-accident, or as part of routine workplace safety measures.\n",
      "distance 84.38806915283203\n",
      "\n",
      "4: 9.\tDiscipline and Termination Policy\n",
      "distance 88.36438751220703\n",
      "\n",
      "5: Monitoring: The company retains the right to monitor internet and email usage for security and compliance purposes.\n",
      "distance 90.28714752197266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the top 5 relevant contexts retrieved based on the search\n",
    "\n",
    "print(\"Top 5 relevant contexts:\")\n",
    "\n",
    "# Iterate over the indices of the top 5 contexts (I[0]) and their corresponding distances (D[0])\n",
    "for i, idx in enumerate(I[0]):\n",
    "    # Print the index (i+1) and the corresponding paragraph from the paragraphs list\n",
    "    print(f\"{i+1}: {paragraphs[idx]}\")\n",
    "    \n",
    "    # Print the distance associated with the context to show its relevance\n",
    "    print(f\"Distance: {D[0][i]}\\n\")  # D[0][i] represents the distance for the i-th context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-Context Retrieval and Text Generation Using Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T06:43:00.580416Z",
     "iopub.status.busy": "2025-02-25T06:43:00.580069Z",
     "iopub.status.idle": "2025-02-25T06:43:00.592322Z",
     "shell.execute_reply": "2025-02-25T06:43:00.591675Z",
     "shell.execute_reply.started": "2025-02-25T06:43:00.580379Z"
    }
   },
   "outputs": [],
   "source": [
    "def search_relevant_contexts(question, question_tokenizer, question_encoder, index, k=5):\n",
    "    \"\"\"\n",
    "    Searches for the most relevant contexts to a given question.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Distances and indices of the top k relevant contexts.\n",
    "    \"\"\"\n",
    "    # Tokenize the question\n",
    "    question_inputs = question_tokenizer(question, return_tensors='pt')\n",
    "\n",
    "    # Encode the question to get the embedding\n",
    "    question_embedding = question_encoder(**question_inputs).pooler_output.detach().numpy()\n",
    "\n",
    "    # Search the index to retrieve top k relevant contexts\n",
    "    D, I = index.search(question_embedding, k)\n",
    "\n",
    "    return D, I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T06:43:00.593390Z",
     "iopub.status.busy": "2025-02-25T06:43:00.593144Z",
     "iopub.status.idle": "2025-02-25T06:43:02.095195Z",
     "shell.execute_reply": "2025-02-25T06:43:02.094538Z",
     "shell.execute_reply.started": "2025-02-25T06:43:00.593365Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T06:43:02.096113Z",
     "iopub.status.busy": "2025-02-25T06:43:02.095882Z",
     "iopub.status.idle": "2025-02-25T06:43:02.099731Z",
     "shell.execute_reply": "2025-02-25T06:43:02.098849Z",
     "shell.execute_reply.started": "2025-02-25T06:43:02.096092Z"
    }
   },
   "outputs": [],
   "source": [
    "contexts= \"What is a large language model?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T06:43:02.100930Z",
     "iopub.status.busy": "2025-02-25T06:43:02.100680Z",
     "iopub.status.idle": "2025-02-25T06:43:02.116728Z",
     "shell.execute_reply": "2025-02-25T06:43:02.115932Z",
     "shell.execute_reply.started": "2025-02-25T06:43:02.100898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[2061,  318,  257, 1588, 3303, 2746,   30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(contexts, return_tensors='pt', max_length=1024, truncation=True)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T07:15:52.564771Z",
     "iopub.status.busy": "2025-02-25T07:15:52.564397Z",
     "iopub.status.idle": "2025-02-25T07:15:55.374177Z",
     "shell.execute_reply": "2025-02-25T07:15:55.373277Z",
     "shell.execute_reply.started": "2025-02-25T07:15:52.564742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2061,   318,   257,  1588,  3303,  2746,    30,   198,   198,    32,\n",
       "          1588,  3303,  2746,   318,   257,   900,   286,  3173,   326,  6901,\n",
       "           703,   257,  3303,   815, 17438,   287,   257,  1813,  4732,    13,\n",
       "           198,   198,    32,  1588,  3303,  2746,   318,   257,   900,   286,\n",
       "          3173,   326,  6901,   703,   257,  3303,   815, 17438,   287,   257]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a summary or output based on the input tokens\n",
    "# The `model.generate()` method generates text based on the input_ids from the tokenized input\n",
    "\n",
    "summary_ids = model.generate(\n",
    "    inputs['input_ids'],  # The tokenized input text\n",
    "    max_length=50,        # Set the maximum length of the generated output to 50 tokens\n",
    "    num_beams=4,          # Use beam search with 4 beams to generate more diverse and high-quality outputs\n",
    "    early_stopping=True,  # Stop the generation early once a complete output is generated\n",
    "    pad_token_id=tokenizer.eos_token_id  # Use the EOS (End of Sentence) token for padding during generation\n",
    ")\n",
    "\n",
    "# Output the generated token IDs\n",
    "summary_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T06:43:05.038540Z",
     "iopub.status.busy": "2025-02-25T06:43:05.038123Z",
     "iopub.status.idle": "2025-02-25T06:43:05.043791Z",
     "shell.execute_reply": "2025-02-25T06:43:05.042918Z",
     "shell.execute_reply.started": "2025-02-25T06:43:05.038504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a large language model?\n",
      "\n",
      "A large language model is a set of rules that describe how a language should behave in a given context.\n",
      "\n",
      "A large language model is a set of rules that describe how a language should behave in a\n"
     ]
    }
   ],
   "source": [
    "# Decode the generated token IDs back into a human-readable text\n",
    "# The `summary_ids[0]` contains the tokenized output, and `skip_special_tokens=True` ensures that special tokens (like [CLS], [SEP]) are excluded from the decoded text\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated summary\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Generation with and without Context Using GPT-2 and Context Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T06:43:05.044959Z",
     "iopub.status.busy": "2025-02-25T06:43:05.044668Z",
     "iopub.status.idle": "2025-02-25T06:43:05.061056Z",
     "shell.execute_reply": "2025-02-25T06:43:05.060324Z",
     "shell.execute_reply.started": "2025-02-25T06:43:05.044905Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_answer_without_context(question):\n",
    "    # Tokenize the input question\n",
    "    inputs = tokenizer(question, return_tensors='pt', max_length=1024, truncation=True)\n",
    "    \n",
    "    # Generate output directly from the question without additional context\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=150, min_length=40, length_penalty=2.0,\n",
    "                                 num_beams=4, early_stopping=True,pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    # Decode and return the generated text\n",
    "    answer = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T06:43:05.062179Z",
     "iopub.status.busy": "2025-02-25T06:43:05.061922Z",
     "iopub.status.idle": "2025-02-25T06:43:15.939283Z",
     "shell.execute_reply": "2025-02-25T06:43:15.938352Z",
     "shell.execute_reply.started": "2025-02-25T06:43:05.062148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: what is mobile policy?\n",
      "\n",
      "Mobile policy is a set of rules and regulations that govern the use of mobile phones and other electronic devices in the United States. Mobile policy is a set of rules and regulations that govern the use of mobile phones and other electronic devices in the United States. Mobile policy is a set of rules and regulations that govern the use of mobile phones and other electronic devices in the United States. Mobile policy is a set of rules and regulations that govern the use of mobile phones and other electronic devices in the United States. Mobile policy is a set of rules and regulations that govern the use of mobile phones and other electronic devices in the United States. Mobile policy is a set of rules and regulations that govern the use of mobile phones and other\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Generating an answer for a given question without providing any context\n",
    "\n",
    "# Define the question for which an answer is to be generated\n",
    "question = \"What is mobile policy?\"\n",
    "\n",
    "# Call the function to generate an answer without passing any context\n",
    "answer = generate_answer_without_context(question)\n",
    "\n",
    "# Print the generated answer\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T06:43:15.940650Z",
     "iopub.status.busy": "2025-02-25T06:43:15.940272Z",
     "iopub.status.idle": "2025-02-25T06:43:15.945096Z",
     "shell.execute_reply": "2025-02-25T06:43:15.944300Z",
     "shell.execute_reply.started": "2025-02-25T06:43:15.940614Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_answer(question, contexts):\n",
    "    \"\"\"\n",
    "    Generates an answer to the given question based on the provided contexts using GPT-2.\n",
    "    \n",
    "    Args:\n",
    "    - question (str): The question for which the answer is to be generated.\n",
    "    - contexts (list): A list of context passages that are relevant to the question.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The generated answer to the question.\n",
    "    \"\"\"\n",
    "    # Concatenate the question with the contexts to form the complete input text for GPT-2\n",
    "    input_text = question + ' ' + ' '.join(contexts)  # Combine question and contexts\n",
    "    \n",
    "    # Tokenize the input text for GPT-2 with truncation to fit the model's max length\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', max_length=1024, truncation=True)\n",
    "\n",
    "    # Generate an answer using GPT-2, controlling output length and other settings\n",
    "    summary_ids = model.generate(\n",
    "        inputs['input_ids'],                  # The tokenized input text\n",
    "        max_new_tokens=50,                     # Limit the number of new tokens generated\n",
    "        min_length=40,                         # Minimum length of the generated output\n",
    "        length_penalty=2.0,                    # Penalty for longer sequences (discourages overly long answers)\n",
    "        num_beams=4,                           # Beam search for better output quality (higher = more expensive)\n",
    "        early_stopping=True,                   # Stop early when the model has generated a complete answer\n",
    "        pad_token_id=tokenizer.eos_token_id    # Use the EOS token as padding\n",
    "    )\n",
    "    \n",
    "    # Decode the generated token IDs into a human-readable string, skipping special tokens (like [CLS], [SEP])\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T07:05:27.241667Z",
     "iopub.status.busy": "2025-02-25T07:05:27.241327Z",
     "iopub.status.idle": "2025-02-25T07:05:27.315954Z",
     "shell.execute_reply": "2025-02-25T07:05:27.315024Z",
     "shell.execute_reply.started": "2025-02-25T07:05:27.241641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph indices: [[44 56 24 13  5]]\n"
     ]
    }
   ],
   "source": [
    "# Define the question to be used in the search for relevant contexts\n",
    "question = \"What is mobile policy?\"\n",
    "\n",
    "# Perform a search to retrieve the top k relevant context paragraphs based on the question\n",
    "# `search_relevant_contexts` returns two outputs: \n",
    "#   - The first output is ignored (`_`) \n",
    "#   - The second output `I` contains the indices of the top k relevant contexts\n",
    "_, I = search_relevant_contexts(question, question_tokenizer, question_encoder, index, k=5)\n",
    "\n",
    "# Print the indices of the top k relevant paragraphs for the given question\n",
    "print(f\"Paragraph indices: {I}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T07:04:50.992648Z",
     "iopub.status.busy": "2025-02-25T07:04:50.992279Z",
     "iopub.status.idle": "2025-02-25T07:04:50.997246Z",
     "shell.execute_reply": "2025-02-25T07:04:50.996546Z",
     "shell.execute_reply.started": "2025-02-25T07:04:50.992616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top contexts: ['Disposal of Smoking Materials: Properly dispose of cigarette butts and related materials in designated receptacles. Littering on company premises is prohibited.', 'Feedback: Candidates will receive timely and constructive feedback on their application and interview performance.', 'Confidentiality: Avoid transmitting sensitive company information via unsecured messaging apps or emails. Be discreet when discussing company matters in public spaces.', 'Review and Update: This policy is subject to regular review and update to remain aligned with evolving legal requirements and best practices in preventing discrimination and harassment. This organization considers it a collective responsibility to ensure a workplace free from discrimination and harassment, and it is essential that every individual within the organization plays their part in upholding these principles.', 'Consequences: Non-compliance with this policy may lead to disciplinary actions, including the potential loss of mobile phone privileges.']\n"
     ]
    }
   ],
   "source": [
    "# Use the indices in I[0] to retrieve the top context paragraphs from the `paragraphs` list\n",
    "# I[0] contains the indices of the most relevant context paragraphs based on some retrieval process.\n",
    "top_contexts = [paragraphs[idx] for idx in I[0]]\n",
    "\n",
    "# Print the top contexts that were retrieved for further inspection\n",
    "print(f\"Top contexts: {top_contexts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T07:04:03.097722Z",
     "iopub.status.busy": "2025-02-25T07:04:03.097363Z",
     "iopub.status.idle": "2025-02-25T07:04:08.158405Z",
     "shell.execute_reply": "2025-02-25T07:04:08.157428Z",
     "shell.execute_reply.started": "2025-02-25T07:04:03.097690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer: what is mobile policy? 4.\tMobile Phone Policy The Mobile Phone Policy sets forth the standards and expectations governing the appropriate and responsible usage of mobile devices in the organization. The purpose of this policy is to ensure that employees utilize mobile phones in a manner consistent with company values and legal compliance. Monitoring: The company retains the right to monitor internet and email usage for security and compliance purposes. Acceptable Use: Mobile devices are primarily intended for work-related tasks. Limited personal usage is allowed, provided it does not disrupt work obligations. The Mobile Phone Policy is aimed at promoting the responsible and secure use of mobile devices in line with legal and ethical standards. Every employee is expected to comprehend and abide by these guidelines. Regular reviews of the policy ensure its ongoing alignment with evolving technology and security best practices.\n",
      "\n",
      "The Mobile Phone Policy sets forth the standards and expectations governing the appropriate and responsible usage of mobile devices in the organization. The purpose of this policy is to ensure that employees utilize mobile phones in a manner consistent with company values and legal compliance. Monitoring\n"
     ]
    }
   ],
   "source": [
    "# Assume `I[0]` contains indices of the top context passages retrieved for the given question\n",
    "# These contexts will be used to generate the answer.\n",
    "\n",
    "# Generate an answer based on the question and the top retrieved contexts\n",
    "answer = generate_answer(question, top_contexts)\n",
    "\n",
    "# Print the generated answer for the given question\n",
    "print(\"Generated Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6703513,
     "sourceId": 10800617,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
