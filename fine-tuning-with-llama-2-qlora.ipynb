{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning with Llama 2, Bits and Bytes, and QLoRA\n",
    "\n",
    "Today we'll explore fine-tuning the Llama 2 model available on Kaggle Models using QLoRA, Bits and Bytes, and PEFT.\n",
    "\n",
    "- QLoRA: [Quantized Low Rank Adapters](https://arxiv.org/pdf/2305.14314.pdf) - this is a method for fine-tuning LLMs that uses a small number of quantized, updateable parameters to limit the complexity of training. This technique also allows those small sets of parameters to be added efficiently into the model itself, which means you can do fine-tuning on lots of data sets, potentially, and swap these \"adapters\" into your model when necessary.\n",
    "- [Bits and Bytes](https://github.com/TimDettmers/bitsandbytes): An excellent package by Tim Dettmers et al., which provides a lightweight wrapper around custom CUDA functions that make LLMs go faster - optimizers, matrix mults, and quantization. In this notebook we'll be using the library to load our model as efficiently as possible.\n",
    "- [PEFT](https://github.com/huggingface/peft): An excellent Huggingface library that enables a number Parameter Efficient Fine-tuning (PEFT) methods, which again make it less expensive to fine-tune LLMs - especially on more lightweight hardware like that present in Kaggle notebooks.\n",
    "\n",
    "Many thanks to [Bojan Tunguz](https://www.kaggle.com/tunguz) for his excellent [Jeopardy dataset](https://www.kaggle.com/datasets/tunguz/200000-jeopardy-questions)!\n",
    "\n",
    "This notebook is based on [an excellent example from LangChain](https://github.com/asokraju/LangChainDatasetForge/blob/main/Finetuning_Falcon_7b.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIa8WRIHvuZy"
   },
   "source": [
    "## Package Installation\n",
    "\n",
    "Note that we're loading very specific versions of these libraries. Dependencies in this space can be quite difficult to untangle, and simply taking the latest version of each library can lead to conflicting version requirements. It's a good idea to take note of which versions work for your particular use case, and `pip install` them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-03-24T14:26:55.451556Z",
     "iopub.status.busy": "2025-03-24T14:26:55.451159Z",
     "iopub.status.idle": "2025-03-24T14:28:45.712419Z",
     "shell.execute_reply": "2025-03-24T14:28:45.711211Z",
     "shell.execute_reply.started": "2025-03-24T14:26:55.451526Z"
    },
    "id": "g3agEMJEnKsl",
    "outputId": "56ab843a-e9c8-4158-8c04-18d77aec44fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -qqq bitsandbytes==0.39.0\n",
    "!pip install -qqq torch==2.0.1\n",
    "!pip install -qqq -U git+https://github.com/huggingface/transformers.git@e03a9cc\n",
    "!pip install -qqq -U git+https://github.com/huggingface/peft.git@42a184f\n",
    "!pip install -qqq -U git+https://github.com/huggingface/accelerate.git@c9fbb71\n",
    "!pip install -qqq datasets==2.12.0\n",
    "!pip install -qqq loralib==0.1.1\n",
    "!pip install -qqq einops==0.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:28:45.714832Z",
     "iopub.status.busy": "2025-03-24T14:28:45.714516Z",
     "iopub.status.idle": "2025-03-24T14:28:45.720829Z",
     "shell.execute_reply": "2025-03-24T14:28:45.720017Z",
     "shell.execute_reply.started": "2025-03-24T14:28:45.714807Z"
    },
    "id": "dv3aJo8Anhyw",
    "outputId": "66f7b274-28a9-45b4-c0e6-d25194424594"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset, Dataset\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgqJriqjwMyK"
   },
   "source": [
    "# Loading and preparing our model\n",
    "\n",
    "We're going to use the Llama 2 7B model for our test. We'll be using Bits and Bytes to load it in 4-bit format, which should reduce memory consumption considerably, at a cost of some accuracy.\n",
    "\n",
    "Note the parameters in `BitsAndBytesConfig` - this is a fairly standard 4-bit quantization configuration, loading the weights in 4-bit format, using a straightforward format (`normal float 4`) with double quantization to improve QLoRA's resolution. The weights are converted back to `bfloat16` for weight updates, then the extra precision is discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:28:45.721929Z",
     "iopub.status.busy": "2025-03-24T14:28:45.721703Z",
     "iopub.status.idle": "2025-03-24T14:28:52.382273Z",
     "shell.execute_reply": "2025-03-24T14:28:52.381574Z",
     "shell.execute_reply.started": "2025-03-24T14:28:45.721909Z"
    },
    "id": "mllA2Ka_ol13"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c04b5dc805443a8b1a5e778a79d6e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#model = \"/kaggle/input/llama-2/pytorch/13b-chat-hf/1\"\n",
    "\n",
    "model_name = \"huggyllama/llama-7b\"\n",
    "model=model_name\n",
    "MODEL_NAME = model_name\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we'll use a nice PEFT wrapper to set up our model for training / fine-tuning. Specifically this function sets the output embedding layer to allow gradient updates, as well as performing some type casting on various components to ensure the model is ready to be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:28:52.385472Z",
     "iopub.status.busy": "2025-03-24T14:28:52.384838Z",
     "iopub.status.idle": "2025-03-24T14:28:52.388988Z",
     "shell.execute_reply": "2025-03-24T14:28:52.388193Z",
     "shell.execute_reply.started": "2025-03-24T14:28:52.385427Z"
    }
   },
   "outputs": [],
   "source": [
    "#model=model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:28:52.391049Z",
     "iopub.status.busy": "2025-03-24T14:28:52.390197Z",
     "iopub.status.idle": "2025-03-24T14:28:52.409102Z",
     "shell.execute_reply": "2025-03-24T14:28:52.408404Z",
     "shell.execute_reply.started": "2025-03-24T14:28:52.390992Z"
    },
    "id": "na8DUq4IoqpB"
   },
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define some helper functions - their purpose is to properly identify our update layers so we can... update them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:28:52.410144Z",
     "iopub.status.busy": "2025-03-24T14:28:52.409916Z",
     "iopub.status.idle": "2025-03-24T14:28:52.421514Z",
     "shell.execute_reply": "2025-03-24T14:28:52.420846Z",
     "shell.execute_reply.started": "2025-03-24T14:28:52.410125Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def get_num_layers(model):\n",
    "    numbers = set()\n",
    "    for name, _ in model.named_parameters():\n",
    "        for number in re.findall(r'\\d+', name):\n",
    "            numbers.add(int(number))\n",
    "    return max(numbers)\n",
    "\n",
    "def get_last_layer_linears(model):\n",
    "    names = []\n",
    "    \n",
    "    num_layers = get_num_layers(model)\n",
    "    for name, module in model.named_modules():\n",
    "        if str(num_layers) in name and not \"encoder\" in name:\n",
    "            if isinstance(module, torch.nn.Linear):\n",
    "                names.append(name)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LORA config\n",
    "\n",
    "Some key elements from this configuration:\n",
    "1. `r` is the width of the small update layer. In theory, this should be set wide enough to capture the complexity of the problem you're attempting to fine-tune for. More simple problems may be able to get away with smaller `r`. In our case, we'll go very small, largely for the sake of speed.\n",
    "2. `target_modules` is set using our helper functions - every layer identified by that function will be included in the PEFT update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:28:52.422625Z",
     "iopub.status.busy": "2025-03-24T14:28:52.422391Z",
     "iopub.status.idle": "2025-03-24T14:28:54.194828Z",
     "shell.execute_reply": "2025-03-24T14:28:54.194069Z",
     "shell.execute_reply.started": "2025-03-24T14:28:52.422605Z"
    },
    "id": "C4Qk3fGLoraw"
   },
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=2,\n",
    "    lora_alpha=32,\n",
    "    target_modules=get_last_layer_linears(model),\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load some data\n",
    "\n",
    "Here, we're loading a 200,000 question Jeopardy dataset. In the interests of time we won't load all of them - just the first 1000 - but we'll fine-tune our model using the question and answers. Note that what we're training the model to do is use its existing knowledge (plus whatever little it learns from our question-answer pairs) to answer questions in the *format* we want, specifically short answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:28:54.196132Z",
     "iopub.status.busy": "2025-03-24T14:28:54.195870Z",
     "iopub.status.idle": "2025-03-24T14:28:54.233447Z",
     "shell.execute_reply": "2025-03-24T14:28:54.232826Z",
     "shell.execute_reply.started": "2025-03-24T14:28:54.196110Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/200000-jeopardy-questions/JEOPARDY_CSV.csv\", nrows=1000)\n",
    "df.columns = [str(q).strip() for q in df.columns]\n",
    "\n",
    "data = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:28:54.234541Z",
     "iopub.status.busy": "2025-03-24T14:28:54.234332Z",
     "iopub.status.idle": "2025-03-24T14:28:54.240210Z",
     "shell.execute_reply": "2025-03-24T14:28:54.239329Z",
     "shell.execute_reply.started": "2025-03-24T14:28:54.234523Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"For the last 8 years of his life, Galileo was under house arrest for espousing this man's theory\",\n",
       "       'No. 2: 1912 Olympian; football star at Carlisle Indian School; 6 MLB seasons with the Reds, Giants & Braves',\n",
       "       'The city of Yuma in this state has a record average of 4,055 hours of sunshine each year',\n",
       "       'In 1963, live on \"The Art Linkletter Show\", this company served its billionth burger',\n",
       "       'Signer of the Dec. of Indep., framer of the Constitution of Mass., second President of the United States'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Question\"].values[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:28:54.243945Z",
     "iopub.status.busy": "2025-03-24T14:28:54.243712Z",
     "iopub.status.idle": "2025-03-24T14:28:54.254229Z",
     "shell.execute_reply": "2025-03-24T14:28:54.253382Z",
     "shell.execute_reply.started": "2025-03-24T14:28:54.243925Z"
    },
    "id": "_Pb9RA5NovNS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For the last 8 years of his life, Galileo was under house arrest for espousing this man's theory. Answer as briefly as possible:\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = df[\"Question\"].values[0] + \". Answer as briefly as possible: \".strip()\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHYgWlyvwy4E"
   },
   "source": [
    "## Let's generate!\n",
    "\n",
    "Below we're setting up our generative model:\n",
    "- Top P: a method for choosing from among a selection of most probable outputs, as opposed to greedily just taking the highest)\n",
    "- Temperature: a modulation on the softmax function used to determine the values of our outputs\n",
    "- We limit the return sequences to 1 - only one answer is allowed! - and deliberately force the answer to be short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:28:54.255532Z",
     "iopub.status.busy": "2025-03-24T14:28:54.255232Z",
     "iopub.status.idle": "2025-03-24T14:28:54.264319Z",
     "shell.execute_reply": "2025-03-24T14:28:54.263466Z",
     "shell.execute_reply.started": "2025-03-24T14:28:54.255503Z"
    },
    "id": "YiqCdCD2oyPH"
   },
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 10\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll generate an answer to our first question, just to see how the model does!\n",
    "\n",
    "It's fascinatingly wrong. :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:28:54.265712Z",
     "iopub.status.busy": "2025-03-24T14:28:54.265384Z",
     "iopub.status.idle": "2025-03-24T14:28:58.701378Z",
     "shell.execute_reply": "2025-03-24T14:28:58.700554Z",
     "shell.execute_reply.started": "2025-03-24T14:28:54.265682Z"
    },
    "id": "o2ELFG0no1xR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the last 8 years of his life, Galileo was under house arrest for espousing this man's theory. Answer as briefly as possible:\n",
      "A. Nicolaus Copernicus\n",
      "B\n",
      "CPU times: user 4.41 s, sys: 16.5 ms, total: 4.43 s\n",
      "Wall time: 4.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "device = \"cuda\"\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids = encoding.input_ids,\n",
    "        attention_mask = encoding.attention_mask,\n",
    "        generation_config = generation_config\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAe7n7T4jP-D"
   },
   "source": [
    "## Format our fine-tuning data\n",
    "\n",
    "We'll match the prompt setup we used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:28:58.702560Z",
     "iopub.status.busy": "2025-03-24T14:28:58.702305Z",
     "iopub.status.idle": "2025-03-24T14:28:59.122946Z",
     "shell.execute_reply": "2025-03-24T14:28:59.121969Z",
     "shell.execute_reply.started": "2025-03-24T14:28:58.702538Z"
    },
    "id": "lm60o2_No7Jz"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"\n",
    "            {data_point[\"Question\"]}. \n",
    "            Answer as briefly as possible: {data_point[\"Answer\"]}\n",
    "            \"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
    "    return tokenized_full_prompt\n",
    "\n",
    "data = data.shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCrTXUqXk0S9"
   },
   "source": [
    "## Train!\n",
    "\n",
    "Now, we'll use our data to update our model. Using the Huggingface `transformers` library, let's set up our training loop and then run it. Note that we are ONLY making one pass on all this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:28:59.124267Z",
     "iopub.status.busy": "2025-03-24T14:28:59.123977Z",
     "iopub.status.idle": "2025-03-24T14:53:18.745091Z",
     "shell.execute_reply": "2025-03-24T14:53:18.744140Z",
     "shell.execute_reply.started": "2025-03-24T14:28:59.124244Z"
    },
    "id": "PGneIe1xpUJV",
    "outputId": "8cdac9ac-d6bf-4d8f-b954-febf7f140591"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 24:13, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=2.28421484375, metrics={'train_runtime': 1459.588, 'train_samples_per_second': 0.685, 'train_steps_per_second': 0.171, 'total_flos': 783792195628032.0, 'train_loss': 2.28421484375, 'epoch': 1.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    output_dir=\"finetune_jeopardy\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.01,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data,\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQRUzbH9oaqG"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and using the model later\n",
    "\n",
    "Now, we'll save the PEFT fine-tuned model, then load it and use it to generate some more answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:53:18.746870Z",
     "iopub.status.busy": "2025-03-24T14:53:18.746527Z",
     "iopub.status.idle": "2025-03-24T14:53:26.940288Z",
     "shell.execute_reply": "2025-03-24T14:53:26.939522Z",
     "shell.execute_reply.started": "2025-03-24T14:53:18.746839Z"
    },
    "id": "Vmce-aSesAHV",
    "outputId": "4bf93e78-2a0b-404c-8b05-3748db1bdc52"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f36ddef3804431be71c685de6f666c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.save_pretrained(\"trained-model\")\n",
    "\n",
    "PEFT_MODEL = \"/kaggle/working/trained-model\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = PeftModel.from_pretrained(model, PEFT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:53:26.941625Z",
     "iopub.status.busy": "2025-03-24T14:53:26.941357Z",
     "iopub.status.idle": "2025-03-24T14:53:26.946037Z",
     "shell.execute_reply": "2025-03-24T14:53:26.945225Z",
     "shell.execute_reply.started": "2025-03-24T14:53:26.941601Z"
    },
    "id": "vgIHyPUasD0b"
   },
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 10\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:53:26.947821Z",
     "iopub.status.busy": "2025-03-24T14:53:26.947261Z",
     "iopub.status.idle": "2025-03-24T14:53:26.956257Z",
     "shell.execute_reply": "2025-03-24T14:53:26.955615Z",
     "shell.execute_reply.started": "2025-03-24T14:53:26.947790Z"
    },
    "id": "I--juWjcCGpS"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-24T14:53:26.957872Z",
     "iopub.status.busy": "2025-03-24T14:53:26.957306Z",
     "iopub.status.idle": "2025-03-24T14:53:31.332486Z",
     "shell.execute_reply": "2025-03-24T14:53:31.331502Z",
     "shell.execute_reply.started": "2025-03-24T14:53:26.957842Z"
    },
    "id": "63Zxai-isGhJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In which sport are barani, rudolph, and randolph all techniques?. Answer as briefly as possible: a) basketball b) football c) baseball d\n",
      "CPU times: user 4.33 s, sys: 6.8 ms, total: 4.34 s\n",
      "Wall time: 4.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = \"In which sport are barani, rudolph, and randolph all techniques?. Answer as briefly as possible: \".strip()\n",
    "\n",
    "device = \"cuda\"\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.inference_mode():\n",
    "  outputs = model.generate(\n",
    "      input_ids = encoding.input_ids,\n",
    "      attention_mask = encoding.attention_mask,\n",
    "      generation_config = generation_config\n",
    "  )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 412543,
     "sourceId": 789660,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 735,
     "modelInstanceId": 3093,
     "sourceId": 4298,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 735,
     "modelInstanceId": 3097,
     "sourceId": 4302,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30559,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
